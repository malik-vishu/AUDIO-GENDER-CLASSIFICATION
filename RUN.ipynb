{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29ea29e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vishwas\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#!pip install torchaudio==0.13.0\n",
    "#!pip install torch==1.13.0\n",
    "import pyaudio\n",
    "import wave\n",
    "import time\n",
    "import os\n",
    "import whisper\n",
    "from MODEL import GenderDetect_V01\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchaudio\n",
    "import soundfile\n",
    "import random\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "class PRIDICTION:\n",
    "    def __init__(self,audio_dir,transformation,target_sample_rate,num_samples,device):\n",
    "        self.audio_dir = audio_dir\n",
    "        self.device = device\n",
    "        self.transformation = transformation.to(self.device)\n",
    "        self.target_sample_rate = target_sample_rate\n",
    "        self.num_samples = num_samples\n",
    "            \n",
    "        \n",
    "\n",
    "    def __getitem__(self, audio_dir):\n",
    "\n",
    "        signal, sr = torchaudio.load(audio_dir)\n",
    "        #print(\"Signal Load:\",signal)\n",
    "        signal = signal.to(self.device)\n",
    "        #print(signal.shape)\n",
    "        signal = random.uniform(2,3)*signal\n",
    "        signal = self._resample_if_necessary(signal, sr)\n",
    "        signal = self._mix_down_if_necessary(signal)\n",
    "        signal = self._cut_if_necessary(signal)\n",
    "                \n",
    "        signal = self._right_pad_if_necessary(signal)\n",
    "        signal = self.transformation(signal)\n",
    "        #print(type(signal))\n",
    "        return signal\n",
    "\n",
    "            \n",
    "    def _resample_if_necessary(self, signal, sr):\n",
    "        if sr != self.target_sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(sr, self.target_sample_rate)\n",
    "            signal = resampler(signal)\n",
    "        return signal\n",
    "            \n",
    "    def _mix_down_if_necessary(self, signal):\n",
    "        if signal.shape[0] > 1:\n",
    "            signal = torch.mean(signal, dim=0, keepdim=True)\n",
    "        return signal\n",
    "\n",
    "    def _cut_if_necessary(self, signal):\n",
    "        if signal.shape[1] > self.num_samples:\n",
    "            signal = signal[:, :self.num_samples]\n",
    "        return signal\n",
    "\n",
    "    def _right_pad_if_necessary(self, signal):\n",
    "        length_signal = signal.shape[1]\n",
    "        if length_signal < self.num_samples:\n",
    "            num_missing_samples = self.num_samples - length_signal\n",
    "            last_dim_padding = (0, num_missing_samples)\n",
    "            signal = torch.nn.functional.pad(signal, last_dim_padding)\n",
    "        return signal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8c24eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_user_input(audio_dir,transformation,target_sample_rate,num_samples,device):\n",
    "    #AUDIO_DIR = audio_dir\n",
    "    \n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "            \n",
    "\n",
    "\n",
    "    mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
    "                sample_rate=22050*3,\n",
    "                n_fft=1024,\n",
    "                hop_length=512,\n",
    "                n_mels=80\n",
    "            )\n",
    "\n",
    "    user_input = PRIDICTION(audio_dir,mel_spectrogram,22050*3,22050*3,device)\n",
    "    \n",
    "            \n",
    "\n",
    "    class_mapping = [\"female\",\n",
    "                \"male\"]        \n",
    "\n",
    "\n",
    "    def predict(model, input,class_mapping):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            predictions = model(input)\n",
    "                    \n",
    "           \n",
    "            predicted_index = predictions[0].argmax(0)\n",
    "            predicted = class_mapping[predicted_index]\n",
    "\n",
    "        return predicted\n",
    "\n",
    "\n",
    "            \n",
    "    GD = GenderDetect_V01(input_shape=1,   \n",
    "                  output_shape=len(class_mapping))\n",
    "    state_dict_saved = torch.load(\"model_0_state.pth\")\n",
    "\n",
    "    GD.load_state_dict(state_dict_saved)\n",
    "\n",
    "\n",
    "    input = user_input.__getitem__(audio_dir)   # [batch size, num_channels, fr, time]\n",
    "    input.unsqueeze_(0)\n",
    "    predicted = predict(GD, input,class_mapping)\n",
    "    print(f\"Predicted: '{predicted}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71d04212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter 1.To record\n",
      "2.To select an existing file:2\n",
      "Enter path of 'wav' file(format eg->D:\\PyTorch_Try_02\\AUDIO\u0001.wav):D:\\PyTorch_Try_02\\AUDIO\\test\\female\\B4_1.wav\n",
      "Predicted: 'male'\n",
      "Do want to transcribe:y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vishwas\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\whisper\\transcribe.py:78: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " And my Furby Boom Tai, which is a toy that lives with me like my pet. All the inspiration around me gives me ideas and motivates me to keep doing something unique. Like, to write songs with my own, to make my school notes on cool post-its, to write songs to learn my faith in school topics and to decorate my books.\n",
      "Do you want to try again??(Y/N):n\n",
      "Thank You!!\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
    "                sample_rate=22050*3,\n",
    "                n_fft=1024,\n",
    "                hop_length=512,\n",
    "                n_mels=80\n",
    "            )\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "        \n",
    "    SAMPLE_RATE = 22050*3\n",
    "    NUM_SAMPLES = 22050*3\n",
    "\n",
    "    choice = int(input(\"\\nEnter 1.To record\\n2.To select an existing file:\"))\n",
    "    if choice == 1:\n",
    "\n",
    "        try:\n",
    "            serial = open(\"Record_Serial.txt\",\"r\")\n",
    "            s = serial.read()\n",
    "        except:\n",
    "            serial = open(\"Record_Serial.txt\",\"w\")\n",
    "            serial.write(\"1\")\n",
    "            s = 1\n",
    "            s = str(s)\n",
    "        finally:\n",
    "            serial.close()\n",
    "        file = s+\".wav\"\n",
    "        path = os.path.join(\"D:\\PyTorch_Try_02\\AUDIO\\input\",file) \n",
    "        \n",
    "        audio = pyaudio.PyAudio()\n",
    "\n",
    "        stream = audio.open(format = pyaudio.paInt16,channels = 1,rate = 44100,input= True, frames_per_buffer = 1024)\n",
    "        frames = []\n",
    "        try:\n",
    "            while True:\n",
    "                data = stream.read(1024)\n",
    "                frames.append(data)\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            pass\n",
    "        wf = wave.open(path,\"wb\")\n",
    "        wf.setnchannels(1)\n",
    "        wf.setsampwidth(audio.get_sample_size(pyaudio.paInt16))\n",
    "        wf.setframerate(44100)\n",
    "        wf.writeframes(b''.join(frames))\n",
    "\n",
    "        stream.stop_stream()\n",
    "        stream.close()\n",
    "        wf.close()\n",
    "        audio.terminate()\n",
    "\n",
    "        serial = open(\"Record_Serial.txt\",\"w\")\n",
    "        s = int(s)\n",
    "        s = s+1\n",
    "        s = str(s)\n",
    "        serial.write(s)\n",
    "        serial.close()\n",
    "\n",
    "        #predict call    \n",
    "        predict_user_input(path,mel_spectrogram,SAMPLE_RATE,NUM_SAMPLES,device)\n",
    "        \n",
    "        transcribe = input(\"Do want to transcribe:\")\n",
    "        if transcribe == 'Y' or transcribe == 'y':\n",
    "            model = whisper.load_model(\"base\")\n",
    "            result = model.transcribe(path)\n",
    "            print(result[\"text\"])\n",
    "        \n",
    "\n",
    "    else:\n",
    "        path = input(\"Enter path of 'wav' file(format eg->D:\\Folder_Name\\1.wav):\")\n",
    "        if os.path.exists(path):\n",
    "        #predict call\n",
    "            #print(path)\n",
    "            predict_user_input(path,mel_spectrogram,SAMPLE_RATE,NUM_SAMPLES,device)\n",
    "            transcribe = input(\"Do want to transcribe:\")\n",
    "            if transcribe == 'Y' or transcribe == 'y':\n",
    "                model = whisper.load_model(\"base\")\n",
    "                result = model.transcribe(path)\n",
    "                print(result[\"text\"])\n",
    "\n",
    "        else:\n",
    "            print(\"Invalid Path!!\")\n",
    "    conti = str(input(\"Do you want to try again??(Y/N):\"))\n",
    "    if conti != \"Y\" and conti != \"y\":\n",
    "        print(\"Thank You!!\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ce7c9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21717c35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
